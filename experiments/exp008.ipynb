{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Library\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/マイドライブ/signate_MUFJ2023/\")\n",
    "from MUFJ.utils import get_score, seed_everything\n",
    "from MUFJ.preprocessing import CustomOrdinalEncoder\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  CFG\n",
    "# ===============================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    max_depth = 6\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    num_cores = 4\n",
    "    data_dir = \"G:/マイドライブ/signate_MUFJ2023/data/\"\n",
    "    save_dir = \"G:/マイドライブ/signate_MUFJ2023/exp/\"\n",
    "    filename = \"exp008\"\n",
    "    numerical_features = [\n",
    "        \"amount\", 'cards_issued', 'credit_limit','year_pin_last_changed','current_age','retirement_age','birth_year','birth_month', 'latitude', 'longitude',\n",
    "        'per_capita_income_zipcode', 'yearly_income_person', 'total_debt','fico_score', 'num_credit_cards', 'expires_month','expires_year','acct_open_date_month', \n",
    "        'acct_open_date_year', \"NonFraudAvgAmount_per_user_card\", \"merchant_id_count_per_user\",\n",
    "    ]\n",
    "        \n",
    "    categorical_features = [\n",
    "        \"errors?\", 'merchant_id', 'merchant_city','merchant_state','zip',\"mcc\",'use_chip','card_brand','card_type', 'has_chip','gender', 'city', 'state', 'zipcode',\n",
    "        \"card_id\", \"user_id\", \"same_zipcode_as_zip\", \"city_is_not_America\", \n",
    "        ]\n",
    "    target_cols = [\"is_fraud?\"]\n",
    "    threshold_per_user = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Utils\n",
    "# ===============================================================\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Data Loading\n",
    "# ===============================================================\n",
    "# load data\n",
    "train = pl.read_csv(CFG.data_dir+\"train.csv\")\n",
    "test = pl.read_csv(CFG.data_dir+\"test.csv\")\n",
    "card = pl.read_csv(CFG.data_dir+\"card.csv\")\n",
    "user = pl.read_csv(CFG.data_dir+\"user.csv\")\n",
    "if CFG.debug:\n",
    "    train = train.sample(n=10000, seed=CFG.seed)\n",
    "    test = test.sample(n=1000, seed=CFG.seed)\n",
    "\n",
    "# add flag\n",
    "train = train.with_columns(\n",
    "    pl.lit(\"train\").alias(\"flag\")\n",
    ")\n",
    "test = test.with_columns(\n",
    "    [\n",
    "        pl.lit(None, dtype=pl.Int64).alias(\"is_fraud?\"),\n",
    "        pl.lit(\"test\").alias(\"flag\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# concat\n",
    "all_data = pl.concat([train, test], how=\"align\")\n",
    "\n",
    "# merge\n",
    "all_data = all_data.join(\n",
    "    card, on=[\"user_id\", \"card_id\"], how=\"left\"\n",
    ")\n",
    "all_data = all_data.join(\n",
    "    user, on=\"user_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Preprocessing\n",
    "# ===============================================================\n",
    "def preprocessing(all_data: pl.DataFrame) -> pl.DataFrame:\n",
    "    all_data = all_data.with_columns(\n",
    "        [   \n",
    "            # str -> float\n",
    "            pl.col(\"amount\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"total_debt\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"credit_limit\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"yearly_income_person\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"per_capita_income_zipcode\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            \n",
    "            # str -> Datetime\n",
    "            pl.col(\"expires\").str.strptime(dtype=pl.Date, format=\"%m/%Y\"),\n",
    "            pl.col(\"acct_open_date\").str.strptime(dtype=pl.Date, format=\"%m/%Y\"),\n",
    "            \n",
    "            # user_id + card_id\n",
    "            (pl.col(\"user_id\").cast(pl.Utf8) + \"-\" + pl.col(\"card_id\").cast(pl.Utf8)).alias(\"user_card_id\"),\n",
    "            \n",
    "            # bool\n",
    "            (pl.col(\"zip\") == pl.col(\"zipcode\")).alias(\"same_zipcode_as_zip\"),\n",
    "            #(pl.col(\"merchant_city\") == \"ONLINE\").alias(\"city_is_ONLINE\"),\n",
    "            pl.when((pl.col(\"merchant_city\").is_null())&(pl.col(\"merchant_city\") != \"ONLINE\")) ## TODO: 上手くまとめられないかな\n",
    "            .then(pl.lit(True))\n",
    "            .otherwise(pl.lit(False))\n",
    "            .alias(\"city_is_not_America\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            # Datetime -> Month, Year\n",
    "            pl.col(\"expires\").dt.year().suffix(\"_year\"),\n",
    "            pl.col(\"expires\").dt.month().suffix(\"_month\"),\n",
    "            pl.col(\"acct_open_date\").dt.year().suffix(\"_year\"),\n",
    "            pl.col(\"acct_open_date\").dt.month().suffix(\"_month\"),\n",
    "            \n",
    "            # fold\n",
    "            pl.lit(None).alias(\"fold\"),\n",
    "            \n",
    "            # feature_engineering\n",
    "            #(pl.col(\"amount\") - pl.col(\"credit_limit\")).cast(pl.Float64).alias(\"remaining_credit\"),\n",
    "            #(2023 - pl.col('year_pin_last_changed')).alias(\"YearsSincePinChange\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return all_data\n",
    "all_data = preprocessing(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  Cross Validation\n",
    "# ===================================================================\n",
    "def kfold(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.sort(\"index\")\n",
    "    df = df.with_columns(pl.Series(range(len(df))).alias(\"id\"))\n",
    "    skf = MultilabelStratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "    for i, (_, val) in enumerate(skf.split(X=df, y=df[[\"is_fraud?\", \"card_id\"]])):\n",
    "        df = df.with_columns(\n",
    "            pl.when(pl.col(\"id\").is_in(val))\n",
    "            .then(pl.lit(i))\n",
    "            .otherwise(pl.col(\"fold\"))\n",
    "            .alias(\"fold\")\n",
    "        )\n",
    "    #print(df[\"fold\"].value_counts())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFG.n_splits = 50でも使えそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Preprocessing_per_fold\n",
    "# ===============================================================\n",
    "def preprocessing_per_fold(CFG, train: pl.DataFrame, test: pl.DataFrame, fold:int):\n",
    "    # data split\n",
    "    X_train = train.filter(pl.col(\"fold\") != fold)\n",
    "    X_valid = train.filter(pl.col(\"fold\") == fold)\n",
    "    test_df = test.clone()\n",
    "    \n",
    "    # user_card_idごとの不正利用があったとき、無かったときのそれぞれの取引金額の平均\n",
    "    tmp = X_train.groupby(by=[\"user_card_id\", \"is_fraud?\"], maintain_order=True).agg(\n",
    "        pl.col(\"amount\").mean()\n",
    "    )\n",
    "    tmp_1 = tmp.filter(pl.col(\"is_fraud?\") == 1).rename({\"amount\":\"FraudAvgAmount_per_user_card\"})[[\"user_card_id\", \"FraudAvgAmount_per_user_card\"]]\n",
    "    tmp_0 = tmp.filter(pl.col(\"is_fraud?\") == 0).rename({\"amount\":\"NonFraudAvgAmount_per_user_card\"})[[\"user_card_id\", \"NonFraudAvgAmount_per_user_card\"]]\n",
    "    \n",
    "    X_train = X_train.join(\n",
    "        tmp_0, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    X_train = X_train.join(\n",
    "        tmp_1, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    \n",
    "    X_valid = X_valid.join(\n",
    "        tmp_0, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    X_valid = X_valid.join(\n",
    "        tmp_1, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    \n",
    "    test_df = test_df.join(\n",
    "        tmp_0, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    test_df = test_df.join(\n",
    "        tmp_1, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # count_encoding\n",
    "    for col in [\"merchant_id\"]:\n",
    "        # per user\n",
    "        count_map = X_train.groupby(by=[\"user_id\", col], maintain_order=True).count().rename(\n",
    "            {\"count\":f\"{col}_count_per_user\"})\n",
    "        X_train = X_train.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        \n",
    "        # per user&card\n",
    "        count_map = X_train.groupby(by=[\"user_id\", \"card_id\", col], maintain_order=True).count().rename(\n",
    "            {\"count\":f\"{col}_count_per_user_card\"})\n",
    "        X_train = X_train.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        \n",
    "    # OrdinalEncoder\n",
    "    oe = CustomOrdinalEncoder(encoded_missing_value=-1)\n",
    "    X_train = pl.concat([X_train, \n",
    "                        oe.fit_transform(X_train[CFG.categorical_features])\n",
    "                        ], how=\"horizontal\")\n",
    "    X_valid = pl.concat([X_valid, \n",
    "                        oe.transform(X_valid[CFG.categorical_features])\n",
    "                        ], how=\"horizontal\")\n",
    "    test_df = pl.concat([test_df, \n",
    "                        oe.transform(test_df[CFG.categorical_features])\n",
    "                        ], how=\"horizontal\")\n",
    "    \n",
    "    \n",
    "    return X_train, X_valid, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  model\n",
    "# ===============================================================\n",
    "lgb_param = {\n",
    "    \"task\":\"train\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\":\"gbdt\",\n",
    "    \"num_iterations\": 10000, # default: 100\n",
    "    \"learning_rate\": 0.05, # default: 0.1\n",
    "    \"num_leaves\": int((2**CFG.max_depth) * 0.7), # max number of leaves in one tree\n",
    "    \"max_depth\": CFG.max_depth, # default -1, int: limit the max depth for tree model  ### xgboost, catboostに合わせる\n",
    "    \"min_child_weight\":1e-3, # double: minimal sum hessian in one leaf\n",
    "    \"min_data_in_leaf\":20, # minimal number of data in one leaf\n",
    "    \"alpha\":0.9, # double, constraints, alpha > 0.0: \n",
    "    \"colsample_bytree\":0.4, # 0 < \"colsample_bytree\" < 1\n",
    "    #: LightGBM will randomly select a subset of features on each iteration (tree) if feature_fraction is smaller than 1.0\n",
    "    \"lambda\": 0, #lambda_l2 >= 0.0: L2 regularization\n",
    "    \"subsample\":1, #0.0 < bagging_fraction <= 1.0\n",
    "    \"num_threads\": CFG.num_cores,\n",
    "    \"metric\": 'binary_logloss',\n",
    "    \"seed\" : CFG.seed,\n",
    "    \"verbosity\": -1, \n",
    "}\n",
    "\n",
    "CFG.use_features = CFG.numerical_features + [col+\"_category\" for col in CFG.categorical_features]\n",
    "\n",
    "\n",
    "def train_lgb_per_user(CFG, train, test):\n",
    "    # kfold\n",
    "    train = kfold(train)\n",
    "    preds = []\n",
    "    oof_df = pl.DataFrame()\n",
    "    for fold in range(CFG.n_splits):\n",
    "        X_train, X_valid, test_df = preprocessing_per_fold(CFG, train, test, fold)\n",
    "        categorical_features = [col for col in CFG.use_features if \"_category\" in col]\n",
    "        lgb_train = lgb.Dataset(X_train[CFG.use_features].to_pandas(), X_train[CFG.target_cols].to_pandas(), categorical_feature = categorical_features,)\n",
    "        lgb_valid = lgb.Dataset(X_valid[CFG.use_features].to_pandas(), X_valid[CFG.target_cols].to_pandas(), categorical_feature = categorical_features,)\n",
    "        model = lgb.train(\n",
    "                        lgb_param, \n",
    "                        lgb_train, \n",
    "                        valid_sets=[lgb_valid],\n",
    "                        categorical_feature = categorical_features,\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                                   #lgb.log_evaluation(period=200)\n",
    "                                   ],\n",
    "                        )\n",
    "        \n",
    "        # valid\n",
    "        X_valid = X_valid.with_columns(\n",
    "            pl.Series(model.predict(X_valid[CFG.use_features].to_pandas(), num_iteration=model.best_iteration)).alias(\"pred_\")\n",
    "        )        \n",
    "        # oof\n",
    "        oof_df = pl.concat(\n",
    "            [oof_df, X_valid]\n",
    "        )\n",
    "        # predict\n",
    "        preds.append(model.predict(test_df[CFG.use_features].to_pandas(), num_iteration=model.best_iteration))\n",
    "        \n",
    "    test_df = test_df.with_columns(\n",
    "        pl.Series(np.mean(preds, axis=0)).alias(\"pred_\")\n",
    "    )\n",
    "    \n",
    "    if CFG.threshold_per_user:\n",
    "        _, threshold = get_score(y_true=oof_df[\"is_fraud?\"], y_pred=oof_df[\"pred_\"], step=0.01, return_threshold=True, disable=True)\n",
    "    \n",
    "        test_df = test_df.with_columns(\n",
    "            pl.when(pl.col(\"pred_\") > threshold)\n",
    "            .then(1)\n",
    "            .otherwise(0)\n",
    "            .alias(\"pred\")\n",
    "        )\n",
    "        \n",
    "        oof_df = oof_df.with_columns(\n",
    "            pl.when(pl.col(\"pred_\") > threshold)\n",
    "            .then(1)\n",
    "            .otherwise(0)\n",
    "            .alias(\"pred\")\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return oof_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c412c1ea0ebc4e61bdc08cda1736582f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.rename() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m      9\u001b[0m train \u001b[39m=\u001b[39m all_data\u001b[39m.\u001b[39mfilter(\n\u001b[0;32m     10\u001b[0m     (pl\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mflag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m&\u001b[39m(pl\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m user)\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m test \u001b[39m=\u001b[39m all_data\u001b[39m.\u001b[39mfilter(\n\u001b[0;32m     13\u001b[0m     (pl\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mflag\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m&\u001b[39m(pl\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m user)\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m _oof_df, _test_df \u001b[39m=\u001b[39m train_lgb_per_user(CFG, train, test)\n\u001b[0;32m     17\u001b[0m oof_df \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mconcat([oof_df, _oof_df])\n\u001b[0;32m     18\u001b[0m test_df \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mconcat([test_df, _test_df])\n",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m, in \u001b[0;36mtrain_lgb_per_user\u001b[1;34m(CFG, train, test)\u001b[0m\n\u001b[0;32m     32\u001b[0m oof_df \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(CFG\u001b[39m.\u001b[39mn_splits):\n\u001b[1;32m---> 34\u001b[0m     X_train, X_valid, test_df \u001b[39m=\u001b[39m preprocessing_per_fold(CFG, train, test, fold)\n\u001b[0;32m     35\u001b[0m     categorical_features \u001b[39m=\u001b[39m [col \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m CFG\u001b[39m.\u001b[39muse_features \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_category\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m col]\n\u001b[0;32m     36\u001b[0m     lgb_train \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_train[CFG\u001b[39m.\u001b[39muse_features]\u001b[39m.\u001b[39mto_pandas(), X_train[CFG\u001b[39m.\u001b[39mtarget_cols]\u001b[39m.\u001b[39mto_pandas(), categorical_feature \u001b[39m=\u001b[39m categorical_features,)\n",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m, in \u001b[0;36mpreprocessing_per_fold\u001b[1;34m(CFG, train, test, fold)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39m# count_encoding\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mmerchant_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# per user\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     count_map \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39;49mgroupby(by\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39muser_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, col])\u001b[39m.\u001b[39;49mcount()\u001b[39m.\u001b[39;49mrename(columns\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mcount\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mcol\u001b[39m}\u001b[39;49;00m\u001b[39m_count_per_user\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[0;32m     43\u001b[0m     X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mmerge(count_map, on\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m, col], how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m     X_valid \u001b[39m=\u001b[39m X_valid\u001b[39m.\u001b[39mmerge(count_map, on\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m, col], how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.rename() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "#  train\n",
    "# ===============================================================\n",
    "test_df = pl.DataFrame()\n",
    "oof_df = pl.DataFrame()\n",
    "\n",
    "for user in tqdm(all_data[\"user_id\"].unique(maintain_order=True)):\n",
    "    # data split\n",
    "    train = all_data.filter(\n",
    "        (pl.col(\"flag\") == \"train\")&(pl.col(\"user_id\") == user)\n",
    "    )\n",
    "    test = all_data.filter(\n",
    "        (pl.col(\"flag\") == \"test\")&(pl.col(\"user_id\") == user)\n",
    "    )\n",
    "    \n",
    "    _oof_df, _test_df = train_lgb_per_user(CFG, train, test)\n",
    "    oof_df = pl.concat([oof_df, _oof_df])\n",
    "    test_df = pl.concat([test_df, _test_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84b587e54ba41439cb882f17d86ae0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m====== CV score ======\u001b[0m\n",
      "\u001b[32m0.6651096184663643 (threshold: 0.33)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  CV score\n",
    "# ===================================================================\n",
    "best_score, threshold = get_score(oof_df[CFG.target_cols], oof_df[\"pred_\"], step=0.005, return_threshold=True, disable=False, )\n",
    "print('\\033[32m'+\"====== CV score ======\"+'\\033[0m')\n",
    "print('\\033[32m'+f'{best_score} (threshold: {threshold})'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6650350009413455 (threshold: 0.335)  \n",
    "0.6651491553851684 (threshold: 0.335)  \n",
    "0.6651380856233531 (threshold: 0.335)  \n",
    "0.6651837656560127 (threshold: 0.335)  \n",
    "0.6651096184663643 (threshold: 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_splits  \n",
    "-> 5にしたらCVは0.6650696416960407 (threshold: 0.335)に。(?分)  \n",
    "-> 10にしたらCVは0.6704852553752019 (threshold: 0.33)に。(4分)  \n",
    "-> 25にしたらCVは0.6747956356671342 (threshold: 0.34)に。(10分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>pred</th></tr><tr><td>i64</td><td>i32</td></tr></thead><tbody><tr><td>471283</td><td>0</td></tr><tr><td>471284</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "┌────────┬──────┐\n",
       "│ index  ┆ pred │\n",
       "│ ---    ┆ ---  │\n",
       "│ i64    ┆ i32  │\n",
       "╞════════╪══════╡\n",
       "│ 471283 ┆ 0    │\n",
       "│ 471284 ┆ 0    │\n",
       "└────────┴──────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  save_data\n",
    "# ===================================================================\n",
    "\n",
    "# oof_df\n",
    "oof_df = oof_df.sort(\"index\")\n",
    "oof_df[[\"index\", \"pred_\"]].rename({\"pred_\":\"pred\"}).write_csv(CFG.save_dir+f\"oof_df_{CFG.filename}.csv\", has_header=True)\n",
    "\n",
    "# test\n",
    "test_df = test_df.sort(\"index\")\n",
    "test_df = test_df.with_columns(\n",
    "    pl.when(pl.col(\"pred_\") > threshold)\n",
    "    .then(1)\n",
    "    .otherwise(0)\n",
    "    .alias(\"pred\")\n",
    ")\n",
    "test_df[[\"index\", \"pred\"]].write_csv(CFG.save_dir+f\"{CFG.filename}.csv\", has_header=False)\n",
    "test_df[[\"index\", \"pred\"]].head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
