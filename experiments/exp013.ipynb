{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Library\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/マイドライブ/signate_MUFJ2023/\")\n",
    "from MUFJ.utils import get_score, seed_everything\n",
    "from MUFJ.model_selection import kfold\n",
    "#from MUFJ.preprocessing import CustomOrdinalEncoder\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  CFG\n",
    "# ===============================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    patience = 2\n",
    "    max_depth = 3\n",
    "    seed = 42\n",
    "    n_splits = 50\n",
    "    num_cores = 4\n",
    "    data_dir = \"G:/マイドライブ/signate_MUFJ2023/data/\"\n",
    "    save_dir = \"G:/マイドライブ/signate_MUFJ2023/exp/\"\n",
    "    filename = \"exp013\"\n",
    "    numerical_features = [\n",
    "        \"amount\", 'cards_issued', 'credit_limit','year_pin_last_changed','current_age','retirement_age','birth_year','birth_month', 'latitude', 'longitude',\n",
    "        'per_capita_income_zipcode', 'yearly_income_person', 'total_debt','fico_score', 'num_credit_cards', 'expires_month','expires_year','acct_open_date_month', \n",
    "        'acct_open_date_year',\n",
    "        \"NonFraudAvgAmount_per_user_card\", \n",
    "        \"merchant_id_count_encoding\", \n",
    "        \"pred_1\", \"pred_2\", \"pred_3\", \n",
    "    ]\n",
    "        \n",
    "    categorical_features = [\n",
    "        \"errors?\", 'merchant_id', 'merchant_city','merchant_state','zip',\"mcc\",'use_chip','card_brand','card_type', 'has_chip','gender', 'city', 'state', 'zipcode',\n",
    "        \"card_id\", \"user_id\",\n",
    "        \"same_zipcode_as_zip\",\n",
    "        \"city_is_not_America\", \n",
    "        ]\n",
    "    target_cols = [\"is_fraud?\"]\n",
    "    threshold_per_user = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Utils\n",
    "# ===============================================================\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Data Loading\n",
    "# ===============================================================\n",
    "# load data\n",
    "train = pd.read_csv(CFG.data_dir+\"train.csv\")\n",
    "test = pd.read_csv(CFG.data_dir+\"test.csv\")\n",
    "card = pd.read_csv(CFG.data_dir+\"card.csv\")\n",
    "user = pd.read_csv(CFG.data_dir+\"user.csv\")\n",
    "if CFG.debug:\n",
    "    train = train.sample(n=10000, random_state=CFG.seed)\n",
    "    test = test.sample(n=1000, random_state=CFG.seed)\n",
    "    \n",
    "# add flag\n",
    "train[\"flag\"] = \"train\"\n",
    "train = pd.merge(train, pd.read_csv(CFG.save_dir+\"oof_df_exp009.csv\").rename(columns={\"pred\":\"pred_1\"}), on=\"index\", how=\"left\")\n",
    "train = pd.merge(train, pd.read_csv(CFG.save_dir+\"oof_df_exp010.csv\").rename(columns={\"pred\":\"pred_2\"}), on=\"index\", how=\"left\")\n",
    "train = pd.merge(train, pd.read_csv(CFG.save_dir+\"oof_df_exp011.csv\").rename(columns={\"pred\":\"pred_3\"}), on=\"index\", how=\"left\")\n",
    "\n",
    "\n",
    "test[\"flag\"] = \"test\"\n",
    "test = pd.merge(test, pd.read_csv(CFG.save_dir+\"exp009.csv\", header=None, names=[\"index\", \"pred_1\"]), on=\"index\", how=\"left\")\n",
    "test = pd.merge(test, pd.read_csv(CFG.save_dir+\"exp010.csv\", header=None, names=[\"index\", \"pred_2\"]), on=\"index\", how=\"left\")\n",
    "test = pd.merge(test, pd.read_csv(CFG.save_dir+\"exp011.csv\", header=None, names=[\"index\", \"pred_3\"]), on=\"index\", how=\"left\")\n",
    "\n",
    "# merge\n",
    "all_data = pd.concat([train, test])\n",
    "all_data = all_data.merge(card, on=[\"user_id\", \"card_id\"], how=\"left\")\n",
    "all_data = all_data.merge(user, on=\"user_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Preprocessing\n",
    "# ===============================================================\n",
    "def preprocessing(all_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # str -> float\n",
    "    for col in [\"amount\", \"total_debt\", \"credit_limit\", \"yearly_income_person\", \"per_capita_income_zipcode\"]:\n",
    "        all_data[col] = all_data[col].apply(lambda x: x[1:]).astype(float)\n",
    "        \n",
    "    # str -> datetime\n",
    "    for col in [\"expires\", \"acct_open_date\"]:\n",
    "        all_data[col] = pd.to_datetime(all_data[col], format=\"%m/%Y\")\n",
    "        all_data[col+\"_year\"] = all_data[col].dt.year\n",
    "        all_data[col+\"_month\"] = all_data[col].dt.month\n",
    "\n",
    "            \n",
    "    # user_id + card_id\n",
    "    all_data[\"user_card_id\"] = all_data[\"user_id\"].astype(str) + \"-\" + all_data[\"card_id\"].astype(str)\n",
    "    \n",
    "    # bool\n",
    "    all_data[\"same_zipcode_as_zip\"] = (all_data[\"zip\"] == all_data[\"zipcode\"])\n",
    "    all_data[\"city_is_not_America\"] = ((all_data[\"zip\"].isnull())&(all_data[\"merchant_city\"] != \"ONLINE\"))\n",
    "\n",
    "    return all_data\n",
    "all_data = preprocessing(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Preprocessing_per_fold\n",
    "# ===============================================================\n",
    "def preprocessing_per_fold(CFG, train: pd.DataFrame, test: pd.DataFrame, fold:int):\n",
    "    # data split\n",
    "    X_train = train[train[\"fold\"] != fold].reset_index(drop=True)\n",
    "    X_valid = train[train[\"fold\"] == fold].reset_index(drop=True)\n",
    "    test_df = test.copy()\n",
    "    \n",
    "    # user_card_idごとの不正利用があったとき、無かったときのそれぞれの取引金額の平均\n",
    "    tmp = X_train.groupby(by=[\"user_card_id\", \"is_fraud?\"])[\"amount\"].mean().reset_index()\n",
    "    #tmp_0 = tmp[tmp[\"is_fraud?\"] == 1].rename(columns={\"amount\":\"FraudAvgAmount_per_user_card\"})[[\"user_card_id\", \"FraudAvgAmount_per_user_card\"]]\n",
    "    tmp_1 = tmp[tmp[\"is_fraud?\"] == 0].rename(columns={\"amount\":\"NonFraudAvgAmount_per_user_card\"})[[\"user_card_id\", \"NonFraudAvgAmount_per_user_card\"]]\n",
    "    X_train = X_train.merge(tmp_1, on=\"user_card_id\", how=\"left\")\n",
    "    X_valid = X_valid.merge(tmp_1, on=\"user_card_id\", how=\"left\")\n",
    "    test_df = test_df.merge(tmp_1, on=\"user_card_id\", how=\"left\")\n",
    "        \n",
    "    # count_encoding\n",
    "    for col in [\"merchant_id\"]:\n",
    "        count_map = X_train[col].value_counts().to_dict()\n",
    "        X_train[col+\"_count_encoding\"] = X_train[col].map(count_map)\n",
    "        X_valid[col+\"_count_encoding\"] = X_valid[col].map(count_map)\n",
    "        test_df[col+\"_count_encoding\"] = test_df[col].map(count_map)\n",
    "\n",
    "    # OrdinalEncoder: これはfoldごとではなくともよい\n",
    "    oe = OrdinalEncoder(categories=\"auto\",\n",
    "                        handle_unknown=\"use_encoded_value\",\n",
    "                        unknown_value=9999,\n",
    "                        encoded_missing_value=-1, \n",
    "                        )\n",
    "    CFG.categorical_features_ = [feature + \"_category\" for feature in CFG.categorical_features]\n",
    "    X_train[CFG.categorical_features_] = oe.fit_transform(X_train[CFG.categorical_features].values)\n",
    "    X_valid[CFG.categorical_features_] = oe.transform(X_valid[CFG.categorical_features].values)\n",
    "    test_df[CFG.categorical_features_] = oe.transform(test_df[CFG.categorical_features].values)\n",
    "    return X_train, X_valid, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  model\n",
    "# ===============================================================\n",
    "lgb_param = {\n",
    "    \"task\":\"train\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\":\"gbdt\",\n",
    "    \"num_iterations\": 10000, # default: 100\n",
    "    \"learning_rate\": 0.05, # default: 0.1\n",
    "    \"num_leaves\": int((2**CFG.max_depth) * 0.7), # max number of leaves in one tree\n",
    "    \"max_depth\": CFG.max_depth, # default -1, int: limit the max depth for tree model  ### xgboost, catboostに合わせる\n",
    "    \"min_child_weight\":1e-3, # double: minimal sum hessian in one leaf\n",
    "    \"min_data_in_leaf\":20, # minimal number of data in one leaf\n",
    "    \"colsample_bytree\":0.4, # 0 < \"colsample_bytree\" < 1\n",
    "    #: LightGBM will randomly select a subset of features on each iteration (tree) if feature_fraction is smaller than 1.0\n",
    "    \"lambda\": 0, #lambda_l2 >= 0.0: L2 regularization\n",
    "    \"subsample\":1, #0.0 < bagging_fraction <= 1.0\n",
    "    \"num_threads\": CFG.num_cores,\n",
    "    \"metric\": 'binary_logloss',\n",
    "    \"seed\" : CFG.seed,\n",
    "    \"verbosity\": -1, \n",
    "}\n",
    "\n",
    "CFG.use_features = CFG.numerical_features + [col+\"_category\" for col in CFG.categorical_features]\n",
    "\n",
    "\n",
    "def train_lgb_per_user(CFG, train, test):\n",
    "    preds = []\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(CFG.n_splits):\n",
    "        X_train, X_valid, test_df = preprocessing_per_fold(CFG, train, test, fold)\n",
    "        categorical_features = [col for col in CFG.use_features if \"_category\" in col]\n",
    "        lgb_train = lgb.Dataset(X_train[CFG.use_features], X_train[CFG.target_cols], categorical_feature = categorical_features,)\n",
    "        lgb_valid = lgb.Dataset(X_valid[CFG.use_features], X_valid[CFG.target_cols], categorical_feature = categorical_features,)\n",
    "        model = lgb.train(\n",
    "                        lgb_param, \n",
    "                        lgb_train, \n",
    "                        valid_sets=[lgb_valid],\n",
    "                        categorical_feature = categorical_features,\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                                   #lgb.log_evaluation(period=200)\n",
    "                                   ],\n",
    "                        )\n",
    "        \n",
    "        # valid\n",
    "        X_valid[\"_pred\"] = model.predict(X_valid[CFG.use_features], num_iteration=model.best_iteration)\n",
    "        \n",
    "        # oof\n",
    "        oof_df = pd.concat([oof_df, X_valid])\n",
    "        \n",
    "        # predict\n",
    "        preds.append(model.predict(test_df[CFG.use_features], num_iteration=model.best_iteration))\n",
    "        \n",
    "    test_df[\"_pred\"] = np.mean(preds, axis=0)\n",
    "    \n",
    "    if CFG.threshold_per_user:\n",
    "        _, threshold = get_score(y_true=oof_df[\"is_fraud?\"], y_pred=oof_df[\"_pred\"], step=0.01, return_threshold=True, disable=True)\n",
    "        test_df[\"pred\"] = np.where(test_df[\"_pred\"] > threshold, 1, 0)\n",
    "        oof_df[\"pred\"] = np.where(oof_df[\"_pred\"] > threshold, 1, 0)\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return oof_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa896a61572044e6b5584e5aaae58349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making fold:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a4e47ac5374298b744560617ae75c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Check_fold_bias:   0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold of User_id 156 is fold\n",
      "17.0    88\n",
      "29.0    87\n",
      "49.0    87\n",
      "32.0    87\n",
      "12.0    87\n",
      "35.0    87\n",
      "34.0    87\n",
      "16.0    87\n",
      "25.0    87\n",
      "31.0    87\n",
      "7.0     87\n",
      "48.0    87\n",
      "26.0    87\n",
      "10.0    87\n",
      "33.0    87\n",
      "43.0    87\n",
      "27.0    87\n",
      "45.0    87\n",
      "6.0     87\n",
      "9.0     87\n",
      "42.0    87\n",
      "40.0    87\n",
      "5.0     87\n",
      "4.0     87\n",
      "38.0    87\n",
      "23.0    87\n",
      "46.0    87\n",
      "39.0    87\n",
      "11.0    87\n",
      "24.0    87\n",
      "18.0    87\n",
      "2.0     87\n",
      "22.0    87\n",
      "13.0    87\n",
      "0.0     87\n",
      "44.0    87\n",
      "15.0    87\n",
      "3.0     87\n",
      "28.0    87\n",
      "21.0    87\n",
      "14.0    87\n",
      "37.0    87\n",
      "30.0    87\n",
      "1.0     87\n",
      "47.0    87\n",
      "8.0     87\n",
      "36.0    87\n",
      "19.0    86\n",
      "20.0    86\n",
      "41.0    86\n",
      "Name: count, dtype: int64\n",
      "The folds for each user have been correctly allocated.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  Cross Validation\n",
    "# ===================================================================\n",
    "train = all_data[all_data[\"flag\"] == \"train\"].reset_index(drop=True)\n",
    "test = all_data[all_data[\"flag\"] == \"test\"].reset_index(drop=True)\n",
    "train = kfold(CFG, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0a27f4da1148369a0cdadbbd3a37c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================================================\n",
    "#  train\n",
    "# ===============================================================\n",
    "test_df = pd.DataFrame()\n",
    "oof_df = pd.DataFrame()\n",
    "\n",
    "for user in tqdm(all_data[\"user_id\"].unique()):\n",
    "    # train, inference\n",
    "    _oof_df, _test_df = train_lgb_per_user(CFG, train[train[\"user_id\"] == user].reset_index(drop=True), test[test[\"user_id\"] == user].reset_index(drop=True))\n",
    "    \n",
    "    # concat\n",
    "    oof_df = pd.concat([oof_df, _oof_df])\n",
    "    test_df = pd.concat([test_df, _test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfd79262fa947389a6f53b04ea98306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m====== CV score ======\u001b[0m\n",
      "\u001b[32m0.6789977790116603 (threshold: 0.35000000000000003)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  CV score\n",
    "# ===================================================================\n",
    "best_score, threshold = get_score(oof_df[CFG.target_cols], oof_df[\"_pred\"], step=0.005, return_threshold=True, disable=False, )\n",
    "print('\\033[32m'+\"====== CV score ======\"+'\\033[0m')\n",
    "print('\\033[32m'+f'{best_score} (threshold: {threshold})'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.6854378393428929 (threshold: 0.36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| No | CV | description |\n",
    "| - | - | - |\n",
    "| 0 | 0.6605582855582856 (threshold: 0.33) | baseline: 再現性が取れるようになった！ |\n",
    "| 1 | 0.6612461572059738 (threshold: 0.31) | add same_zipcode_as_zip |\n",
    "| 2 | 0.6613739523655713 (threshold: 0.325) | add \"city_is_not_America\" |\n",
    "| 3 | 0.661542928387218 (threshold: 0.33) | add \"NonFraudAvgAmount_per_user_card\" |\n",
    "| 4 | **0.6652354618211794 (threshold: 0.34)** | add \"merchant_id_count_encoding\" |\n",
    "| 5 | 0.6649124932920181 (threshold: 0.35000000000000003) | add \"mcc_count_encoding\" |\n",
    "| 6 | 0.6646481732070365 (threshold: 0.32) | add \"merchant_city_count_encoding\" |\n",
    "| 7 | 0.6647920368485517 (threshold: 0.34500000000000003) | add \"merchant_state_count_encoding\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471283</td>\n",
       "      <td>0.031423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471284</td>\n",
       "      <td>0.108292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      pred\n",
       "0  471283  0.031423\n",
       "0  471284  0.108292"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  save_data\n",
    "# ===================================================================\n",
    "\n",
    "# oof_df\n",
    "oof_df = oof_df.sort_values(\"index\")\n",
    "oof_df[[\"index\", \"_pred\"]].rename(columns={\"_pred\":\"pred\"}).to_csv(CFG.save_dir+f\"oof_df_{CFG.filename}.csv\", index=False)\n",
    "\n",
    "# test\n",
    "test_df = test_df.sort_values(\"index\")\n",
    "test_df[\"pred\"] = np.where(test_df[\"_pred\"] > threshold, 1, 0)\n",
    "\n",
    "test_df[[\"index\", \"_pred\"]].rename(columns={\"_pred\":\"pred\"}).to_csv(CFG.save_dir+f\"{CFG.filename}.csv\", index=False, header=False)\n",
    "test_df[[\"index\", \"_pred\"]].rename(columns={\"_pred\":\"pred\"}).head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
