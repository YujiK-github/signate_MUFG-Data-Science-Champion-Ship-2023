{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Library\n",
    "# ===============================================================\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"G:/マイドライブ/signate_MUFJ2023/\")\n",
    "from MUFJ.utils import get_score, seed_everything\n",
    "from MUFJ.preprocessing import CustomOrdinalEncoder\n",
    "\n",
    "from math import comb\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  CFG\n",
    "# ===============================================================\n",
    "class CFG:\n",
    "    debug = False\n",
    "    seed = 42\n",
    "    n_splits = 5\n",
    "    num_cores = 4\n",
    "    data_dir = \"G:/マイドライブ/signate_MUFJ2023/data/\"\n",
    "    stopping_rounds = 100\n",
    "    save_dir = \"G:/マイドライブ/signate_MUFJ2023/exp/\"\n",
    "    filename = \"exp003\"\n",
    "    numerical_features = [\n",
    "        \"amount\", 'cards_issued', 'credit_limit','year_pin_last_changed','current_age','retirement_age','birth_year','birth_month', 'latitude', 'longitude',\n",
    "        'per_capita_income_zipcode', 'yearly_income_person', 'total_debt','fico_score', 'num_credit_cards', 'expires_month','expires_year','acct_open_date_month', \n",
    "        'acct_open_date_year', \"YearsFromAcctOpenToPinChange\", \"DiffNonFraudAvgAmount_per_user\",\n",
    "    ]\n",
    "        \n",
    "    categorical_features = [\n",
    "        \"errors?\", 'merchant_id', 'merchant_city','merchant_state','zip',\"mcc\",'use_chip','card_brand','card_type', 'has_chip','gender', 'city', 'state', 'zipcode',\n",
    "        \"card_id\", \"user_id\", \"same_zipcode_as_zip\", \"city_is_ONLINE\",\n",
    "        ]\n",
    "    target_cols = [\"is_fraud?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Utils\n",
    "# ===============================================================\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Data Loading\n",
    "# ===============================================================\n",
    "train = pl.read_csv(CFG.data_dir+\"train.csv\")\n",
    "test = pl.read_csv(CFG.data_dir+\"test.csv\")\n",
    "card = pl.read_csv(CFG.data_dir+\"card.csv\")\n",
    "user = pl.read_csv(CFG.data_dir+\"user.csv\")\n",
    "\n",
    "train = train.with_columns(\n",
    "    pl.lit(\"train\").alias(\"flag\")\n",
    ")\n",
    "test = test.with_columns(\n",
    "    [\n",
    "        pl.lit(None, dtype=pl.Int64).alias(\"is_fraud?\"),\n",
    "        pl.lit(\"test\").alias(\"flag\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if CFG.debug:\n",
    "    train = train.sample(n=10000, seed=CFG.seed)\n",
    "    test = test.sample(n=1000, seed=CFG.seed)\n",
    "\n",
    "all_data = pl.concat([train, test], how=\"align\")\n",
    "all_data = all_data.join(\n",
    "    card, on=[\"user_id\", \"card_id\"], how=\"left\"\n",
    ")\n",
    "all_data = all_data.join(\n",
    "    user, on=\"user_id\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Preprocessing\n",
    "# ===============================================================\n",
    "def preprocessing(all_data: pl.DataFrame) -> pl.DataFrame:\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [   \n",
    "            # str -> float\n",
    "            pl.col(\"amount\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"total_debt\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"credit_limit\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"yearly_income_person\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            pl.col(\"per_capita_income_zipcode\").apply(lambda x: x[1:]).cast(pl.Float64),\n",
    "            \n",
    "            # str -> Datetime\n",
    "            pl.col(\"expires\").str.strptime(dtype=pl.Date, format=\"%m/%Y\"),\n",
    "            pl.col(\"acct_open_date\").str.strptime(dtype=pl.Date, format=\"%m/%Y\"),\n",
    "            \n",
    "            # bool\n",
    "            (pl.col(\"zip\") == pl.col(\"zipcode\")).alias(\"same_zipcode_as_zip\"),\n",
    "            #(pl.col(\"state\") == pl.col(\"merchant_state\")).alias(\"same_state\"),\n",
    "            #(pl.col(\"city\") == pl.col(\"merchant_city\")).alias(\"same_city\"),\n",
    "            (pl.col(\"merchant_city\") == \"ONLINE\").alias(\"city_is_ONLINE\"),\n",
    "            #pl.when((pl.col(\"merchant_city\").is_null())&(pl.col(\"merchant_city\") != \"ONLINE\")) ## TODO: 上手くまとめられないかな\n",
    "            #.then(pl.lit(True))\n",
    "            #.otherwise(pl.lit(False))\n",
    "            #.alias(\"city_is_not_America\"),\n",
    "\n",
    "            # user_id + card_id\n",
    "            (pl.col(\"user_id\").cast(pl.Utf8) + \"-\" + pl.col(\"card_id\").cast(pl.Utf8)).alias(\"user_card_id\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            # Datetime -> Month, Year\n",
    "            pl.col(\"expires\").dt.year().suffix(\"_year\"),\n",
    "            pl.col(\"expires\").dt.month().suffix(\"_month\"),\n",
    "            pl.col(\"acct_open_date\").dt.year().suffix(\"_year\"),\n",
    "            pl.col(\"acct_open_date\").dt.month().suffix(\"_month\"),\n",
    "        \n",
    "            # feature_engineering\n",
    "            #(pl.col(\"amount\") - pl.col(\"credit_limit\")).cast(pl.Float64).alias(\"remaining_credit\"),\n",
    "            #(pl.col(\"amount\") / (pl.col(\"yearly_income_person\") + 1e-9)).alias(\"income_transaction_ratio\"),\n",
    "            (pl.col(\"amount\") / (pl.col('per_capita_income_zipcode') + 1e-9)).alias(\"income_transaction_ratio\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    all_data = all_data.with_columns(\n",
    "        [\n",
    "            #(2023 - pl.col('year_pin_last_changed')).alias(\"YearsSincePinChange\"),\n",
    "            (pl.col(\"year_pin_last_changed\") - pl.col(\"acct_open_date_year\")).alias(\"YearsFromAcctOpenToPinChange\"),\n",
    "            #(pl.col(\"retirement_age\") - pl.col(\"current_age\")).alias(\"YearsUntilRetirement\"),\n",
    "            #(pl.col(\"expires_year\") - pl.col(\"year_pin_last_changed\")).alias(\"YearsFromPinChangeToExpires\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return all_data\n",
    "all_data = preprocessing(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>fold</th><th>counts</th></tr><tr><td>i32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>94257</td></tr><tr><td>1</td><td>94257</td></tr><tr><td>2</td><td>94257</td></tr><tr><td>3</td><td>94256</td></tr><tr><td>4</td><td>94256</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌──────┬────────┐\n",
       "│ fold ┆ counts │\n",
       "│ ---  ┆ ---    │\n",
       "│ i32  ┆ u32    │\n",
       "╞══════╪════════╡\n",
       "│ 0    ┆ 94257  │\n",
       "│ 1    ┆ 94257  │\n",
       "│ 2    ┆ 94257  │\n",
       "│ 3    ┆ 94256  │\n",
       "│ 4    ┆ 94256  │\n",
       "└──────┴────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>fold</th><th>is_fraud?</th></tr><tr><td>i32</td><td>struct[2]</td></tr></thead><tbody><tr><td>0</td><td>{1,6526}</td></tr><tr><td>0</td><td>{0,87731}</td></tr><tr><td>1</td><td>{1,6527}</td></tr><tr><td>1</td><td>{0,87730}</td></tr><tr><td>2</td><td>{1,6526}</td></tr><tr><td>2</td><td>{0,87731}</td></tr><tr><td>3</td><td>{0,87730}</td></tr><tr><td>3</td><td>{1,6526}</td></tr><tr><td>4</td><td>{0,87730}</td></tr><tr><td>4</td><td>{1,6526}</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "┌──────┬───────────┐\n",
       "│ fold ┆ is_fraud? │\n",
       "│ ---  ┆ ---       │\n",
       "│ i32  ┆ struct[2] │\n",
       "╞══════╪═══════════╡\n",
       "│ 0    ┆ {1,6526}  │\n",
       "│ 0    ┆ {0,87731} │\n",
       "│ 1    ┆ {1,6527}  │\n",
       "│ 1    ┆ {0,87730} │\n",
       "│ …    ┆ …         │\n",
       "│ 3    ┆ {0,87730} │\n",
       "│ 3    ┆ {1,6526}  │\n",
       "│ 4    ┆ {0,87730} │\n",
       "│ 4    ┆ {1,6526}  │\n",
       "└──────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  Cross Validation\n",
    "# ===================================================================\n",
    "all_data = all_data.with_columns(pl.lit(None).alias(\"fold\"))\n",
    "train = all_data.filter(\n",
    "    pl.col(\"flag\") == \"train\"\n",
    ").sort(by=\"index\")\n",
    "test = all_data.filter(\n",
    "    pl.col(\"flag\") == \"test\"\n",
    ")\n",
    "\n",
    "# すべてのfoldにuser_card_idとis_fraud?がうまくいきわたるようにCVを作る\n",
    "skf = MultilabelStratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "for i, (_, val) in enumerate(skf.split(X=train, y=train[[\"is_fraud?\", \"user_card_id\"]])):\n",
    "    train = train.with_columns(\n",
    "        pl.when(pl.col(\"index\").is_in(val))\n",
    "        .then(pl.lit(i))\n",
    "        .otherwise(pl.col(\"fold\"))\n",
    "        .alias(\"fold\")\n",
    "    )\n",
    "    \n",
    "display(train[\"fold\"].value_counts())\n",
    "display(train.groupby(\"fold\").agg(\n",
    "    pl.col(\"is_fraud?\").value_counts()\n",
    "    ).explode(columns=\"is_fraud?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "#  Preprocessing_per_fold\n",
    "# ===============================================================\n",
    "def preprocessing_per_fold(CFG, train: pl.DataFrame, test: pl.DataFrame, fold:int):\n",
    "    # data split\n",
    "    X_train = train.filter(pl.col(\"fold\") != fold)\n",
    "    X_valid = train.filter(pl.col(\"fold\") == fold)\n",
    "    test_df = test.clone()\n",
    "    \n",
    "    # user_idごとの不正利用があったとき、無かったときのそれぞれの取引金額の平均\n",
    "    tmp = X_train.groupby([\"user_id\", \"is_fraud?\"]).agg(\n",
    "        pl.col(\"amount\").mean()\n",
    "    )\n",
    "    #tmp_1 = tmp.filter(pl.col(\"is_fraud?\") == 1).rename({\"amount\":\"FraudAvgAmount_per_user\"})[[\"user_id\", \"FraudAvgAmount_per_user\"]]\n",
    "    tmp_0 = tmp.filter(pl.col(\"is_fraud?\") == 0).rename({\"amount\":\"NonFraudAvgAmount_per_user\"})[[\"user_id\", \"NonFraudAvgAmount_per_user\"]]\n",
    "    \n",
    "    X_train = X_train.join(\n",
    "        tmp_0, on=\"user_id\", how=\"left\"\n",
    "    )\n",
    "    #X_train = X_train.join(\n",
    "    #    tmp_1, on=\"user_id\", how=\"left\"\n",
    "    #)\n",
    "    \n",
    "    X_valid = X_valid.join(\n",
    "        tmp_0, on=\"user_id\", how=\"left\"\n",
    "    )\n",
    "    #X_valid = X_valid.join(\n",
    "    #    tmp_1, on=\"user_id\", how=\"left\"\n",
    "    #)\n",
    "    \n",
    "    test_df = test_df.join(\n",
    "        tmp_0, on=\"user_id\", how=\"left\"\n",
    "    )\n",
    "    #test_df = test_df.join(\n",
    "    #    tmp_1, on=\"user_id\", how=\"left\"\n",
    "    #)\n",
    "    ## 自分自身との差\n",
    "    tmp = [\n",
    "        #(pl.col(\"amount\") - pl.col(\"FraudAvgAmount_per_user\")).alias(\"DiffFraudAvgAmount_per_user\"),\n",
    "        (pl.col(\"amount\") - pl.col(\"NonFraudAvgAmount_per_user\")).alias(\"DiffNonFraudAvgAmount_per_user\"),\n",
    "    ]\n",
    "    X_train = X_train.with_columns(tmp)\n",
    "    X_valid = X_valid.with_columns(tmp)\n",
    "    test_df = test_df.with_columns(tmp)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # user_card_idごとの不正利用があったとき、無かったときのそれぞれの取引金額の平均\n",
    "    tmp = X_train.groupby([\"user_card_id\", \"is_fraud?\"]).agg(\n",
    "        pl.col(\"amount\").mean()\n",
    "    )\n",
    "    tmp_1 = tmp.filter(pl.col(\"is_fraud?\") == 1).rename({\"amount\":\"FraudAvgAmount_per_user_card_id\"})[[\"user_card_id\", \"FraudAvgAmount_per_user_card_id\"]]\n",
    "    tmp_0 = tmp.filter(pl.col(\"is_fraud?\") == 0).rename({\"amount\":\"NonFraudAvgAmount_per_user_card_id\"})[[\"user_card_id\", \"NonFraudAvgAmount_per_user_card_id\"]]\n",
    "    \n",
    "    X_train = X_train.join(\n",
    "        tmp_0, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    X_train = X_train.join(\n",
    "        tmp_1, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    \n",
    "    X_valid = X_valid.join(\n",
    "        tmp_0, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    X_valid = X_valid.join(\n",
    "        tmp_1, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    \n",
    "    test_df = test_df.join(\n",
    "        tmp_0, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    test_df = test_df.join(\n",
    "        tmp_1, on=\"user_card_id\", how=\"left\"\n",
    "    )\n",
    "    ## 自分自身との差\n",
    "    tmp = [\n",
    "        (pl.col(\"amount\") - pl.col(\"FraudAvgAmount_per_user_card_id\")).alias(\"DiffFraudAvgAmount_per_user_card_id\"),\n",
    "        (pl.col(\"amount\") - pl.col(\"NonFraudAvgAmount_per_user_card_id\")).alias(\"DiffNonFraudAvgAmount_per_user_card_id\"),\n",
    "    ]\n",
    "    X_train = X_train.with_columns(tmp)\n",
    "    X_valid = X_valid.with_columns(tmp)\n",
    "    test_df = test_df.with_columns(tmp)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # user_id, merchant_idごとの不正利用があったとき、無かったときのそれぞれの取引金額の平均\n",
    "    tmp = X_train.groupby([\"user_id\",\"merchant_id\", \"is_fraud?\"]).agg(\n",
    "        pl.col(\"amount\").mean()\n",
    "    )\n",
    "    tmp_1 = tmp.filter(pl.col(\"is_fraud?\") == 1).rename({\"amount\":\"FraudAvgAmount_per_user*merchant_id\"})[[\"user_id\",\"merchant_id\", \"FraudAvgAmount_per_user*merchant_id\"]]\n",
    "    tmp_0 = tmp.filter(pl.col(\"is_fraud?\") == 0).rename({\"amount\":\"NonFraudAvgAmount_per_user*merchant_id\"})[[\"user_id\",\"merchant_id\", \"NonFraudAvgAmount_per_user*merchant_id\"]]\n",
    "    \n",
    "    X_train = X_train.join(\n",
    "        tmp_0, on=[\"user_id\",\"merchant_id\",], how=\"left\"\n",
    "    )\n",
    "    X_train = X_train.join(\n",
    "        tmp_1, on=[\"user_id\",\"merchant_id\",], how=\"left\"\n",
    "    )\n",
    "    \n",
    "    X_valid = X_valid.join(\n",
    "        tmp_0, on=[\"user_id\",\"merchant_id\",], how=\"left\"\n",
    "    )\n",
    "    X_valid = X_valid.join(\n",
    "        tmp_1, on=[\"user_id\",\"merchant_id\",], how=\"left\"\n",
    "    )\n",
    "    \n",
    "    test_df = test_df.join(\n",
    "        tmp_0, on=[\"user_id\",\"merchant_id\",], how=\"left\"\n",
    "    )\n",
    "    test_df = test_df.join(\n",
    "        tmp_1, on=[\"user_id\",\"merchant_id\",], how=\"left\"\n",
    "    )\n",
    "    ## 自分自身との差\n",
    "    tmp = [\n",
    "        (pl.col(\"amount\") - pl.col(\"FraudAvgAmount_per_user*merchant_id\")).alias(\"DiffFraudAvgAmount_per_user*merchant_id\"),\n",
    "        (pl.col(\"amount\") - pl.col(\"NonFraudAvgAmount_per_user*merchant_id\")).alias(\"DiffNonFraudAvgAmount_per_user*merchant_id\"),\n",
    "    ]\n",
    "    X_train = X_train.with_columns(tmp)\n",
    "    X_valid = X_valid.with_columns(tmp)\n",
    "    test_df = test_df.with_columns(tmp)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    for col in [\"merchant_id\"]:\n",
    "        # per user\n",
    "        count_map = X_train.groupby([\"user_id\", col]).count().rename(\n",
    "            {\"count\":f\"{col}_count_per_user\"})\n",
    "        X_train = X_train.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        \n",
    "        # per user&card\n",
    "        count_map = X_train.groupby([\"user_id\", \"card_id\", col]).count().rename(\n",
    "            {\"count\":f\"{col}_count_per_user_card\"})\n",
    "        X_train = X_train.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # target_encoding\n",
    "    # 1変数\n",
    "    for cols in tqdm(combinations(CFG.categorical_features, 1), total=comb(len(CFG.categorical_features), 1), leave=False):\n",
    "        group_cols = list(cols)  # Convert the combination tuple to a list\n",
    "        mean_map = X_train.groupby(group_cols).agg(\n",
    "            pl.col(\"is_fraud?\").mean()\n",
    "        ).rename({\"is_fraud?\": f\"{group_cols[0]}_is_fraud?\"})\n",
    "        X_train = X_train.join(mean_map, on=group_cols, how=\"left\")\n",
    "        X_valid = X_valid.join(mean_map, on=group_cols, how=\"left\")\n",
    "        test_df = test_df.join(mean_map, on=group_cols, how=\"left\")\n",
    "\n",
    "    for cols in tqdm(combinations(CFG.categorical_features, 2), total=comb(len(CFG.categorical_features), 2)):\n",
    "        group_cols = list(cols)  # Convert the combination tuple to a list\n",
    "        mean_map = X_train.groupby(group_cols).agg(\n",
    "            pl.col(\"is_fraud?\").mean()\n",
    "        ).rename({\"is_fraud?\": f\"{group_cols[0]}*{group_cols[1]}_is_fraud?\"})\n",
    "        X_train = X_train.join(mean_map, on=group_cols, how=\"left\")\n",
    "        X_valid = X_valid.join(mean_map, on=group_cols, how=\"left\")\n",
    "        test_df = test_df.join(mean_map, on=group_cols, how=\"left\")\n",
    "\n",
    "            \n",
    "    # 3変数\n",
    "    for cols in tqdm(combinations(CFG.categorical_features, 3), total=comb(len(CFG.categorical_features), 3)):\n",
    "        group_cols = list(cols)  # Convert the combination tuple to a list\n",
    "        mean_map = X_train.groupby(group_cols).agg(\n",
    "            pl.col(\"is_fraud?\").mean()\n",
    "        ).rename({\"is_fraud?\": f\"{group_cols[0]}*{group_cols[1]}*{group_cols[2]}_is_fraud?\"})\n",
    "        X_train = X_train.join(mean_map, on=group_cols, how=\"left\")\n",
    "        X_valid = X_valid.join(mean_map, on=group_cols, how=\"left\")\n",
    "        test_df = test_df.join(mean_map, on=group_cols, how=\"left\")\n",
    "        \n",
    "    # target_encoding\n",
    "    for col in [col for col in CFG.categorical_features if col not in [\"user_id\", \"card_id\"]]:\n",
    "        # per_user\n",
    "        mean_map = X_train.groupby([\"user_id\", col]).agg(\n",
    "            pl.col(\"is_fraud?\").mean()\n",
    "            ).rename({\"is_fraud?\":f\"user_id*{col}_is_fraud?\"})\n",
    "        X_train = X_train.join(mean_map, on=[\"user_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(mean_map, on=[\"user_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(mean_map, on=[\"user_id\", col], how=\"left\")\n",
    "        \n",
    "        # per_user_card\n",
    "        mean_map = X_train.groupby([\"user_id\",\"card_id\", col]).agg(\n",
    "            pl.col(\"is_fraud?\").mean()\n",
    "            ).rename({\"is_fraud?\":f\"user_id*card_id*{col}_is_fraud?\"})\n",
    "        X_train = X_train.join(mean_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(mean_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(mean_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        \n",
    "        \n",
    "    # count_encoding\n",
    "    for col in [col for col in CFG.categorical_features if col not in [\"user_id\", \"card_id\"]]:\n",
    "        # per user\n",
    "        count_map = X_train.groupby([\"user_id\", col]).count().rename(\n",
    "            {\"count\":f\"{col}_count_per_user\"})\n",
    "        X_train = X_train.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=[\"user_id\", col], how=\"left\")\n",
    "        \n",
    "        # per user&card\n",
    "        count_map = X_train.groupby([\"user_id\", \"card_id\", col]).count().rename(\n",
    "            {\"count\":f\"{col}_count_per_user_card\"})\n",
    "        X_train = X_train.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        X_valid = X_valid.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        test_df = test_df.join(count_map, on=[\"user_id\", \"card_id\", col], how=\"left\")\n",
    "        \n",
    "    # frequency_encoding\n",
    "    for col in [col for col in CFG.categorical_features if col not in [\"user_id\", \"card_id\"]]:\n",
    "        # per user_id\n",
    "        tmp = X_train.groupby([\"user_id\", col]).agg(\n",
    "            pl.col(\"is_fraud?\").count()\n",
    "        )\n",
    "        tmp = tmp.join(\n",
    "            X_train.groupby(\"user_id\").count(),\n",
    "            on=\"user_id\", how=\"left\"\n",
    "        )\n",
    "        tmp = tmp.with_columns(\n",
    "            (pl.col(\"is_fraud?\") / pl.col(\"count\")).alias(f\"{col}_freq_per_user\")\n",
    "        )\n",
    "        X_train = X_train.join(\n",
    "            tmp[[\"user_id\", col, f\"{col}_freq_per_user\"]],\n",
    "            on=[\"user_id\", col], how=\"left\"\n",
    "        )\n",
    "        X_valid = X_valid.join(\n",
    "            tmp[[\"user_id\", col, f\"{col}_freq_per_user\"]],\n",
    "            on=[\"user_id\", col], how=\"left\"\n",
    "        )\n",
    "        test_df = test_df.join(\n",
    "            tmp[[\"user_id\", col, f\"{col}_freq_per_user\"]],\n",
    "            on=[\"user_id\", col], how=\"left\"\n",
    "        )\n",
    "        \n",
    "        # per user_id & card_id\n",
    "        tmp = X_train.groupby([\"user_id\", \"card_id\", col]).agg(\n",
    "            pl.col(\"is_fraud?\").count()\n",
    "        )\n",
    "        tmp = tmp.join(\n",
    "            X_train.groupby([\"user_id\", \"card_id\"]).count(),\n",
    "            on=[\"user_id\", \"card_id\"], how=\"left\"\n",
    "        )\n",
    "        tmp = tmp.with_columns(\n",
    "            (pl.col(\"is_fraud?\") / pl.col(\"count\")).alias(f\"{col}_freq_per_user_card\")\n",
    "        )\n",
    "        X_train = X_train.join(\n",
    "            tmp[[\"user_id\", \"card_id\", col, f\"{col}_freq_per_user_card\"]],\n",
    "            on=[\"user_id\", \"card_id\", col], how=\"left\"\n",
    "        )\n",
    "        X_valid = X_valid.join(\n",
    "            tmp[[\"user_id\", \"card_id\", col, f\"{col}_freq_per_user_card\"]],\n",
    "            on=[\"user_id\", \"card_id\", col], how=\"left\"\n",
    "        )\n",
    "        test_df = test_df.join(\n",
    "            tmp[[\"user_id\", \"card_id\", col, f\"{col}_freq_per_user_card\"]],\n",
    "            on=[\"user_id\",  \"card_id\", col], how=\"left\"\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    # OrdinalEncoder\n",
    "    oe = CustomOrdinalEncoder(encoded_missing_value=-1)\n",
    "    X_train = pl.concat([X_train, \n",
    "                        oe.fit_transform(X_train[CFG.categorical_features])\n",
    "                        ], how=\"horizontal\")\n",
    "    X_valid = pl.concat([X_valid, \n",
    "                        oe.transform(X_valid[CFG.categorical_features])\n",
    "                        ], how=\"horizontal\")\n",
    "    test_df = pl.concat([test_df, \n",
    "                        oe.transform(test_df[CFG.categorical_features])\n",
    "                        ], how=\"horizontal\")\n",
    "    \n",
    "    \n",
    "    return X_train, X_valid, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  evaluate\n",
    "# ===================================================================\n",
    "def train_lgb(CFG, lgb_param):\n",
    "    oof_df = pl.DataFrame()\n",
    "    preds = []\n",
    "    for fold in range(CFG.n_splits):\n",
    "        X_train, X_valid, test_df = preprocessing_per_fold(CFG, train, test, fold)\n",
    "        # train\n",
    "        categorical_features = [col for col in CFG.use_features if \"_category\" in col]\n",
    "        lgb_train = lgb.Dataset(X_train[CFG.use_features].to_pandas(), X_train[CFG.target_cols].to_pandas(), categorical_feature = categorical_features,)\n",
    "        lgb_valid = lgb.Dataset(X_valid[CFG.use_features].to_pandas(), X_valid[CFG.target_cols].to_pandas(), categorical_feature = categorical_features,)\n",
    "        model = lgb.train(\n",
    "                        lgb_param, \n",
    "                        lgb_train, \n",
    "                        valid_sets=[lgb_valid],\n",
    "                        categorical_feature = categorical_features,\n",
    "                        callbacks=[lgb.early_stopping(stopping_rounds=CFG.stopping_rounds, verbose=True),\n",
    "                                   lgb.log_evaluation(period=200)],\n",
    "                        )\n",
    "        \n",
    "        # valid\n",
    "        X_valid = X_valid.with_columns(\n",
    "            pl.Series(model.predict(X_valid[CFG.use_features].to_pandas(), num_iteration=model.best_iteration)).alias(\"pred\")\n",
    "        )\n",
    "        #print(f\"fold{fold}:\", get_score(y_true=X_valid[CFG.target_cols], y_pred=X_valid[\"pred\"]))\n",
    "        \n",
    "        # oof\n",
    "        oof_df = pl.concat(\n",
    "            [oof_df, X_valid]\n",
    "        )\n",
    "        \n",
    "        # predict\n",
    "        preds.append(model.predict(test_df[CFG.use_features].to_pandas(), num_iteration=model.best_iteration))\n",
    "        \n",
    "    test_df = test_df.with_columns(\n",
    "        pl.Series(np.mean(preds, axis=0)).alias(\"pred\")\n",
    "    )\n",
    "    score, threshold = get_score(oof_df[CFG.target_cols], oof_df[\"pred\"], step=0.005, return_threshold=True)\n",
    "    return score, threshold, oof_df, test_df\n",
    "\n",
    "\n",
    "def train_xgb(CFG, xgb_param):\n",
    "    oof_df = pl.DataFrame()\n",
    "    preds = []\n",
    "    for fold in range(CFG.n_splits):\n",
    "        X_train, X_valid, test_df = preprocessing_per_fold(CFG, train, test, fold)\n",
    "        d_train = xgb.DMatrix(data=X_train[CFG.use_features].to_numpy(), label=X_train[CFG.target_cols].to_numpy(), enable_categorical=True)\n",
    "        d_valid = xgb.DMatrix(data=X_valid[CFG.use_features].to_numpy(), label=X_valid[CFG.target_cols].to_numpy(), enable_categorical=True)\n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        \n",
    "        # train\n",
    "        model = xgb.train(dtrain=d_train,\n",
    "                          num_boost_round= 1000,\n",
    "                          evals=watchlist,\n",
    "                          early_stopping_rounds=CFG.stopping_rounds,\n",
    "                          verbose_eval=50,\n",
    "                          params=xgb_param)\n",
    "        \n",
    "        # valid\n",
    "        X_valid = X_valid.with_columns(\n",
    "            pl.Series(model.predict(xgb.DMatrix(X_valid[CFG.use_features].to_pandas(), enable_categorical=True), ntree_limit=model.best_ntree_limit)).alias(\"pred\")\n",
    "        )\n",
    "        print(f\"fold{fold}:\", get_score(y_true=X_valid[CFG.target_cols], y_pred=X_valid[\"pred\"]))\n",
    "        \n",
    "        # oof\n",
    "        oof_df = pl.concat(\n",
    "            [oof_df, X_valid]\n",
    "        )\n",
    "        \n",
    "        # predict\n",
    "        #preds.append(model.predict(xgb.DMatrix(test_df[CFG.use_features].to_pandas(), enable_categorical=True), ntree_limit=model.best_ntree_limit))\n",
    "        \n",
    "    #test_df = test_df.with_columns(\n",
    "    #    pl.Series(np.mean(preds, axis=0)).alias(\"pred\")\n",
    "    #)\n",
    "    score, threshold = get_score(oof_df[CFG.target_cols], oof_df[\"pred\"], step=0.005, return_threshold=True)\n",
    "    return score, threshold, oof_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.1398\n",
      "[400]\tvalid_0's binary_logloss: 0.133975\n",
      "[600]\tvalid_0's binary_logloss: 0.131843\n",
      "[800]\tvalid_0's binary_logloss: 0.130667\n",
      "[1000]\tvalid_0's binary_logloss: 0.130154\n",
      "[1200]\tvalid_0's binary_logloss: 0.129845\n",
      "[1400]\tvalid_0's binary_logloss: 0.129695\n",
      "Early stopping, best iteration is:\n",
      "[1468]\tvalid_0's binary_logloss: 0.129578\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.142141\n",
      "[400]\tvalid_0's binary_logloss: 0.135999\n",
      "[600]\tvalid_0's binary_logloss: 0.134072\n",
      "[800]\tvalid_0's binary_logloss: 0.132976\n",
      "[1000]\tvalid_0's binary_logloss: 0.132201\n",
      "[1200]\tvalid_0's binary_logloss: 0.131993\n",
      "[1400]\tvalid_0's binary_logloss: 0.131819\n",
      "Early stopping, best iteration is:\n",
      "[1368]\tvalid_0's binary_logloss: 0.13174\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.141513\n",
      "[400]\tvalid_0's binary_logloss: 0.13547\n",
      "[600]\tvalid_0's binary_logloss: 0.133515\n",
      "[800]\tvalid_0's binary_logloss: 0.132363\n",
      "[1000]\tvalid_0's binary_logloss: 0.131788\n",
      "[1200]\tvalid_0's binary_logloss: 0.131247\n",
      "[1400]\tvalid_0's binary_logloss: 0.131148\n",
      "Early stopping, best iteration is:\n",
      "[1488]\tvalid_0's binary_logloss: 0.131032\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.141625\n",
      "[400]\tvalid_0's binary_logloss: 0.135119\n",
      "[600]\tvalid_0's binary_logloss: 0.13312\n",
      "[800]\tvalid_0's binary_logloss: 0.131758\n",
      "[1000]\tvalid_0's binary_logloss: 0.13102\n",
      "[1200]\tvalid_0's binary_logloss: 0.130695\n",
      "[1400]\tvalid_0's binary_logloss: 0.130473\n",
      "Early stopping, best iteration is:\n",
      "[1452]\tvalid_0's binary_logloss: 0.130414\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\tvalid_0's binary_logloss: 0.140608\n",
      "[400]\tvalid_0's binary_logloss: 0.134656\n",
      "[600]\tvalid_0's binary_logloss: 0.132432\n",
      "[800]\tvalid_0's binary_logloss: 0.131414\n",
      "[1000]\tvalid_0's binary_logloss: 0.130617\n",
      "[1200]\tvalid_0's binary_logloss: 0.130247\n",
      "[1400]\tvalid_0's binary_logloss: 0.130086\n",
      "Early stopping, best iteration is:\n",
      "[1454]\tvalid_0's binary_logloss: 0.129946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4028374c91474e3a99d2506ebcce9054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m====== CV score ======\u001b[0m\n",
      "\u001b[32m0.6532026768642447 (threshold: 0.355)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "#  evaluate\n",
    "# ===================================================================\n",
    "CFG.use_features = CFG.numerical_features + [col+\"_category\" for col in CFG.categorical_features]\n",
    "\n",
    "lgb_param = {\n",
    "    \"task\":\"train\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting\":\"gbdt\",\n",
    "    \"num_iterations\": 10000, # default: 100\n",
    "    \"learning_rate\": 0.05, # default: 0.1\n",
    "    \"num_leaves\": int((2**6) * 0.7), # max number of leaves in one tree\n",
    "    \"max_depth\": 6, # default -1, int: limit the max depth for tree model  ### xgboost, catboostに合わせる\n",
    "    \"min_child_weight\":1e-3, # double: minimal sum hessian in one leaf\n",
    "    \"min_data_in_leaf\":20, # minimal number of data in one leaf\n",
    "    \"alpha\":0.9, # double, constraints, alpha > 0.0: \n",
    "    \"colsample_bytree\":0.4, # 0 < \"colsample_bytree\" < 1\n",
    "    #: LightGBM will randomly select a subset of features on each iteration (tree) if feature_fraction is smaller than 1.0\n",
    "    \"lambda\": 0, #lambda_l2 >= 0.0: L2 regularization\n",
    "    \"subsample\":1, #0.0 < bagging_fraction <= 1.0\n",
    "    \"num_threads\": CFG.num_cores,\n",
    "    \"metric\": 'binary_logloss',\n",
    "    \"seed\" : CFG.seed,\n",
    "    \"verbosity\": -1, \n",
    "}\n",
    "\n",
    "best_score, threshold, oof_df, test_df = train_lgb(CFG, lgb_param)\n",
    "print('\\033[32m'+\"====== CV score ======\"+'\\033[0m')\n",
    "print('\\033[32m'+f'{best_score} (threshold: {threshold})'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| id | description | CV |  |\n",
    "| - | - | - | - |\n",
    "| 0 | baseline | 0.6503219747786423 (threshold: 0.33) | |\n",
    "| 1 | add same_zipcode_as_zip | 0.6522468256956091 (threshold: 0.36) | 採用！ |\n",
    "| 2 | add user_card_id | 0.6477621196981704 (threshold: 0.37) | 不採用 |  \n",
    "| 3 | add city_is_ONLINE | 0.6527855745513311 (threshold: 0.35000000000000003) | 採用！ |\n",
    "| 4 | add city_is_not_America | 0.6527855745513311 (threshold: 0.35000000000000003) | 不採用(これ変わらないのか) |\n",
    "| 5 | add remaining_credit | 0.6522429938135965 (threshold: 0.34500000000000003) | 不採用 |\n",
    "| 6 | add \"YearsSincePinChange\" | 0.6497795839647335 (threshold: 0.36) | 不採用 |\n",
    "| 7 | add \"YearsFromAcctOpenToPinChange\" | **0.6528226893020749** (threshold: 0.34) | 採用 |\n",
    "| 8 | add \"YearsUntilRetirement\" | 0.6520358071036674 (threshold: 0.35000000000000003) | 不採用 |\n",
    "| 9 | add YearsFromPinChangeToExpires | 0.6521451248705454 (threshold: 0.35000000000000003)| 不採用 |\n",
    "| 10 | change 'num_credit_cards' numerical to categorical | 0.651323243426059 (threshold: 0.34500000000000003) | 不採用 |\n",
    "| 11 | add \"same_state\" | 0.6507434409729858 (threshold: 0.34500000000000003) | 不採用 |\n",
    "| 12 | add \"same_city\" | 0.6497568352494373 (threshold: 0.365) | 不採用 |\n",
    "| 13 | add FraudAvgAmount_per_user | 0.6490824157490824 (threshold: 0.325) | 不採用 |\n",
    "| 14 | add \"NonFraudAvgAmount_per_user\" | 0.6524137458457532 (threshold: 0.36) | 不採用 |\n",
    "| 15 | add DiffFraudAvgAmount_per_user | 0.6521343926743872 (threshold: 0.34) | 不採用 |\n",
    "| 16 | add DiffNonFraudAvgAmount_per_user | **0.6531318219983208** (threshold: 0.335) | 採用！ |\n",
    "| 17 | add \"FraudAvgAmount_per_user_card_id\" | 0.6506323435532013 (threshold: 0.37) | 不採用 |\n",
    "| 18 | add \"NonFraudAvgAmount_per_user_card_id | 0.6494871491576404 (threshold: 0.34) | 不採用 |\n",
    "| 19 | add \"DiffFraudAvgAmount_per_user_card_id\" | 0.6497280129742878 (threshold: 0.34) | 不採用 |\n",
    "| 20 | add \"DiffNonFraudAvgAmount_per_user_card_id\" | 0.6509064934181436 (threshold: 0.35000000000000003) | 不採用 |\n",
    "| 21 | add \"FraudAvgAmount_per_user*merchant_id\" | 0.6103046901745978 (threshold: 0.28500000000000003) | 不採用 |\n",
    "| 22 | add \"NonFraudAvgAmount_per_user*merchant_id\" | 0.5241076138561631 (threshold: 0.38) | 不採用 |\n",
    "| 23 | add \"DiffFraudAvgAmount_per_user*merchant_id\" | 0.6083466812173269 (threshold: 0.27) | 不採用 |\n",
    "| 24 | add \"DiffNonFraudAvgAmount_per_user*merchant_id\" | 0.5209444807765119 (threshold: 0.365) | 不採用 |\n",
    "| 25 | add \"merchant_id_count_per_user\" | 0.6525988603004662 (threshold: 0.375) | 不採用(これ変わらないのか) |\n",
    "| 26 | add \"merchant_id_count_per_user_card\" | 0.651119717124095 (threshold: 0.34) | 不採用 |\n",
    "| 27 | n_splits: 5->10 | **0.6551747132568686 (threshold: 0.325)** | 保留(パラメータいじれば0.657くらいはいきそう?) |\n",
    "| 28 | add \"income_transaction_ratio\" | 0.6513817187974492 (threshold: 0.34) | 不採用 |\n",
    "| 28 | add \"income_transaction_ratio\"(ver2) | 0.6497960571040109 (threshold: 0.35000000000000003) | 不採用 |\n",
    "\n",
    "target_encoding, count_encoding, frequency_encodingは後でやりましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_leaves = int((2**max_depth) * 0.7)にした\n",
    "\n",
    "| max_depth | CV |\n",
    "| - | - |\n",
    "| 6 | 0.6508343165346369 (threshold: 0.34) |\n",
    "| 8 | 0.6522853769607017 (threshold: 0.335) |\n",
    "| 10 | 0.6502891162281452 (threshold: 0.34500000000000003) |\n",
    "\n",
    "learning_rateを変更\n",
    "\n",
    "| max_depth | CV |\n",
    "| - | - |\n",
    "| 4 | 0.6514412566008543 (threshold: 0.34) |\n",
    "| 6 | 0.6532026768642447 (threshold: 0.355) |\n",
    "| 8 | 0.6530890498508736 (threshold: 0.34500000000000003) |\n",
    "| 10 | 0.6524644104217029 (threshold: 0.32) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "#  save_data\n",
    "# ===================================================================\n",
    "\n",
    "# oof_df\n",
    "oof_df[[\"index\", \"pred\"]].write_csv(CFG.save_dir+f\"oof_df_{CFG.filename}.csv\", has_header=True)\n",
    "\n",
    "# test\n",
    "test_df = test_df.with_columns(\n",
    "    pl.when(pl.col(\"pred\") > threshold)\n",
    "    .then(1)\n",
    "    .otherwise(0)\n",
    "    .alias(\"pred\")\n",
    ")\n",
    "test_df[[\"index\", \"pred\"]].write_csv(CFG.save_dir+f\"{CFG.filename}.csv\", has_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (457_958, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>pred</th></tr><tr><td>i64</td><td>i32</td></tr></thead><tbody><tr><td>471283</td><td>0</td></tr><tr><td>471284</td><td>0</td></tr><tr><td>471285</td><td>0</td></tr><tr><td>471286</td><td>1</td></tr><tr><td>471287</td><td>0</td></tr><tr><td>471288</td><td>0</td></tr><tr><td>471289</td><td>0</td></tr><tr><td>471290</td><td>0</td></tr><tr><td>471291</td><td>0</td></tr><tr><td>471292</td><td>0</td></tr><tr><td>471293</td><td>1</td></tr><tr><td>471294</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>929229</td><td>0</td></tr><tr><td>929230</td><td>0</td></tr><tr><td>929231</td><td>0</td></tr><tr><td>929232</td><td>0</td></tr><tr><td>929233</td><td>0</td></tr><tr><td>929234</td><td>0</td></tr><tr><td>929235</td><td>0</td></tr><tr><td>929236</td><td>0</td></tr><tr><td>929237</td><td>0</td></tr><tr><td>929238</td><td>0</td></tr><tr><td>929239</td><td>0</td></tr><tr><td>929240</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (457_958, 2)\n",
       "┌────────┬──────┐\n",
       "│ index  ┆ pred │\n",
       "│ ---    ┆ ---  │\n",
       "│ i64    ┆ i32  │\n",
       "╞════════╪══════╡\n",
       "│ 471283 ┆ 0    │\n",
       "│ 471284 ┆ 0    │\n",
       "│ 471285 ┆ 0    │\n",
       "│ 471286 ┆ 1    │\n",
       "│ …      ┆ …    │\n",
       "│ 929237 ┆ 0    │\n",
       "│ 929238 ┆ 0    │\n",
       "│ 929239 ┆ 0    │\n",
       "│ 929240 ┆ 0    │\n",
       "└────────┴──────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[\"index\", \"pred\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCFG.use_features = CFG.numerical_features + [col+\"_category\" for col in CFG.categorical_features] + [col for col in X_train.columns if \"target_mean\" in col]\\n\\nxgb_param = {\\n    \"booster\":\"gbtree\",\\n    \"verbosity\": 1,\\n    \"n_thread\": 4,\\n    \"eta\": 0.3,\\n    \"gamma\": 0,\\n    \"max_depth\": 6,\\n    \"task\": \"train\",\\n    \"objective\": \"binary:logistic\", # binary:logistic, binary:logitraw, binary:hinge\\n    \"eval_metric\": \"logloss\", # auc\\n    \"seed\": CFG.seed,\\n}\\n\\n# The Kaggle Book: Hyperparameter optimization\\n\\nxgb_param = {\\n    #\"n_estimators\": study.suggest_int(\"n_estimators\", 10, 5000),\\n    \"learning_rate\": study.suggest_float(\"learning_rate\", 0.01, 1.0, log=True),\\n    \"min_child_weight\": study.suggest_int(\"min_child_weight\", 1, 10),\\n    \"max_depth\": study.suggest_int(\"max_depth\", 1, 50),\\n    \"max_delta_step\": study.suggest_int(\"max_delta_step\", 0, 20),\\n    \"subsample\": study.suggest_float(\"subsample\", 0.1, 1.0),\\n    \"colsample_bytree\": study.suggest_float(\"colsample_bytree\", 0.1, 10),\\n    \"colsample_bylevel\": study.suggest_float(\"colsample_bylevel\", 0.1, 1.0),\\n    \"reg_lambda\": study.suggest_float(\"reg_lambda\", 1e-9, 100.0, log=True),\\n    \"reg_alpha\": study.suggest_float(\"reg_alpha\", 1e-9, 100.0, log=True),\\n    \"gamma\": study.suggest_float(\"gamma\", 1e-9, 100.0, log=True),\\n    \"scale_pos_weight\", study.suggest_float(\"scale_pos_weight\", 1e-6, 500.0, log=True),\\n    \"task\": \"train\",\\n    \"objective\": \"binary:logistic\", # binary:logistic, binary:logitraw, binary:hinge\\n    \"eval_metric\": \"logloss\", # auc\\n    \"seed\": CFG.seed,\\n    \"task\": \"train\",\\n}\\n\\n\\nxgb_best_score, xgb_threshold, xgb_oof_df, xgb_test_df = train_xgb(CFG, xgb_param)\\nprint(\\'\\x1b[32m\\'+\"====== CV score ======\"+\\'\\x1b[0m\\')\\nprint(\\'\\x1b[32m\\'+f\\'{xgb_best_score} (threshold: {xgb_threshold})\\'+\\'\\x1b[0m\\')\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CFG.use_features = CFG.numerical_features + [col+\"_category\" for col in CFG.categorical_features] + [col for col in X_train.columns if \"target_mean\" in col]\n",
    "\n",
    "xgb_param = {\n",
    "    \"booster\":\"gbtree\",\n",
    "    \"verbosity\": 1,\n",
    "    \"n_thread\": 4,\n",
    "    \"eta\": 0.3,\n",
    "    \"gamma\": 0,\n",
    "    \"max_depth\": 6,\n",
    "    \"task\": \"train\",\n",
    "    \"objective\": \"binary:logistic\", # binary:logistic, binary:logitraw, binary:hinge\n",
    "    \"eval_metric\": \"logloss\", # auc\n",
    "    \"seed\": CFG.seed,\n",
    "}\n",
    "\n",
    "# The Kaggle Book: Hyperparameter optimization\n",
    "\n",
    "xgb_param = {\n",
    "    #\"n_estimators\": study.suggest_int(\"n_estimators\", 10, 5000),\n",
    "    \"learning_rate\": study.suggest_float(\"learning_rate\", 0.01, 1.0, log=True),\n",
    "    \"min_child_weight\": study.suggest_int(\"min_child_weight\", 1, 10),\n",
    "    \"max_depth\": study.suggest_int(\"max_depth\", 1, 50),\n",
    "    \"max_delta_step\": study.suggest_int(\"max_delta_step\", 0, 20),\n",
    "    \"subsample\": study.suggest_float(\"subsample\", 0.1, 1.0),\n",
    "    \"colsample_bytree\": study.suggest_float(\"colsample_bytree\", 0.1, 10),\n",
    "    \"colsample_bylevel\": study.suggest_float(\"colsample_bylevel\", 0.1, 1.0),\n",
    "    \"reg_lambda\": study.suggest_float(\"reg_lambda\", 1e-9, 100.0, log=True),\n",
    "    \"reg_alpha\": study.suggest_float(\"reg_alpha\", 1e-9, 100.0, log=True),\n",
    "    \"gamma\": study.suggest_float(\"gamma\", 1e-9, 100.0, log=True),\n",
    "    \"scale_pos_weight\", study.suggest_float(\"scale_pos_weight\", 1e-6, 500.0, log=True),\n",
    "    \"task\": \"train\",\n",
    "    \"objective\": \"binary:logistic\", # binary:logistic, binary:logitraw, binary:hinge\n",
    "    \"eval_metric\": \"logloss\", # auc\n",
    "    \"seed\": CFG.seed,\n",
    "    \"task\": \"train\",\n",
    "}\n",
    "\n",
    "\n",
    "xgb_best_score, xgb_threshold, xgb_oof_df, xgb_test_df = train_xgb(CFG, xgb_param)\n",
    "print('\\033[32m'+\"====== CV score ======\"+'\\033[0m')\n",
    "print('\\033[32m'+f'{xgb_best_score} (threshold: {xgb_threshold})'+'\\033[0m')\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
